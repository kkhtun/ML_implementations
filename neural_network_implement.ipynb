{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_network_implement.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNl1Z467juzRhsU+aZm/1X0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P2uJWYdMvetR","colab_type":"text"},"source":["This code sheet is my attempt to implement a plain neural network from scratch and proving that hidden layers (including deep neural networks) are essential in learning non-linear functions that we see in most of our real-world data  \n","A neural network without a hidden layer is unable to learn a non-linear XOR function as described below  \n","Reference: www.pyimagesearch.com"]},{"cell_type":"code","metadata":{"id":"Lpv7oCMryQF_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Chsa7CIGcnGu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601023722518,"user_tz":-390,"elapsed":3193,"user":{"displayName":"Khaing Khant Htun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixmBjV-53DLh5NOtpNtJtqnb5nqLzlfWLyD8OE0g=s64","userId":"00234393451589880378"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHBBUCPzcwZ4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1601028802899,"user_tz":-390,"elapsed":1014,"user":{"displayName":"Khaing Khant Htun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixmBjV-53DLh5NOtpNtJtqnb5nqLzlfWLyD8OE0g=s64","userId":"00234393451589880378"}}},"source":["class NeuralNetwork:\n","  def __init__(self, layers, alpha = 0.1):\n","    self.W = []\n","    self.layers = layers\n","    self.alpha = alpha\n","\n","    for i in np.arange(0, len(layers)-2):\n","      w = np.random.randn(layers[i]+1, layers[i+1]+1) / np.sqrt(layers[i])\n","      self.W.append(w)\n","\n","    w = np.random.randn(layers[-2]+1, layers[-1]) / np.sqrt(layers[-2])\n","    self.W.append(w)\n","\n","  def __repr__(self):\n","    return \"Neural Network : {}\".format(\"-\".join(str(l) for l in self.layers))\n","\n","  def sigmoid(self, x):\n","    return 1 / (1 + np.exp(-x))\n","\n","  def d_sigmoid(self, x):\n","    return x * (1 - x)\n","\n","\n","  def fit(self, X, Y, epoch = 1000, displayUpdate = 1000):\n","\n","    X = np.c_[X, np.ones((X.shape[0]))]\n","\n","    for i in np.arange(0, epoch):\n","      for (x, y) in zip(X, Y):\n","        self.fit_partial(x, y)\n","      if i == 0 or (i+1) % displayUpdate == 0:\n","        loss = self.compute_loss(X, Y)\n","        print(\"Epoch {}, Loss {:.7f}\".format(i+1, loss))\n","\n","\n","  def fit_partial(self, x, y):\n","\n","    A = [np.atleast_2d(x)]\n","\n","    for layer_no in np.arange(0, len(self.W)):\n","      \n","      h = A[layer_no].dot(self.W[layer_no])\n","      a = self.sigmoid(h)\n","      A.append(a)\n","\n","    error = A[-1] - y\n","    D = [error * self.d_sigmoid(A[-1])]\n","\n","    for layer_no in np.arange(len(A)-2, 0, -1):\n","      delta = (D[-1].dot(self.W[layer_no].T)) * self.d_sigmoid(A[layer_no])\n","      D.append(delta)\n","\n","    D = D[::-1]\n","\n","    for layer_no in np.arange(0, len(self.W)):\n","      self.W[layer_no] += -self.alpha * A[layer_no].T.dot(D[layer_no])\n","\n","\n","  def predict(self, X, addBias = True):\n","    p = np.atleast_2d(X)\n","\n","    if addBias:\n","      p = np.c_[p, np.ones((p.shape[0]))]\n","    \n","    for layer_no in np.arange(0, len(self.W)):\n","      p = self.sigmoid(np.dot(p, self.W[layer_no]))\n","    return p\n","\n","  def compute_loss(self, X, Y):\n","\n","    Y = np.atleast_2d(Y)\n","    preds = self.predict(X, addBias = False)\n","    loss = np.sum(np.square(preds - Y))/2\n","    return loss"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eg2MdH_Gcwje","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601028805554,"user_tz":-390,"elapsed":1394,"user":{"displayName":"Khaing Khant Htun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixmBjV-53DLh5NOtpNtJtqnb5nqLzlfWLyD8OE0g=s64","userId":"00234393451589880378"}},"outputId":"84826bde-54dc-4365-cde6-389f2bd74f49"},"source":["NeuralNetwork([2,2,1])"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Neural Network : 2-2-1"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"ibZVf3QYuzZT","colab_type":"text"},"source":["Training the neural network on XOR function using a single hidden layer [2-2-1] architecture   \n","Please do note that neural network is able to predict the non-linear XOR function correctly"]},{"cell_type":"code","metadata":{"id":"5Hpfih7peNKk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"ok","timestamp":1601028811339,"user_tz":-390,"elapsed":4213,"user":{"displayName":"Khaing Khant Htun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixmBjV-53DLh5NOtpNtJtqnb5nqLzlfWLyD8OE0g=s64","userId":"00234393451589880378"}},"outputId":"6361b0ad-aead-44cf-8391-30ea8039eff8"},"source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([[0], [1], [1], [0]])\n","nn = NeuralNetwork([2,2,1], alpha = 0.5)\n","nn.fit(X, Y, epoch = 20000)\n","\n","for (x, y) in zip(X,Y):\n","\n","  pred = nn.predict(x)[0][0]\n","  step = 1 if pred > 0.5 else 0\n","  print(\"Data {}, Ground Truth {} Prediction {} Step {}\".format(x, y, pred, step))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Epoch 1, Loss 0.5059337\n","Epoch 1000, Loss 0.3094045\n","Epoch 2000, Loss 0.0083001\n","Epoch 3000, Loss 0.0034353\n","Epoch 4000, Loss 0.0020839\n","Epoch 5000, Loss 0.0014714\n","Epoch 6000, Loss 0.0011281\n","Epoch 7000, Loss 0.0009108\n","Epoch 8000, Loss 0.0007618\n","Epoch 9000, Loss 0.0006538\n","Epoch 10000, Loss 0.0005720\n","Epoch 11000, Loss 0.0005082\n","Epoch 12000, Loss 0.0004569\n","Epoch 13000, Loss 0.0004150\n","Epoch 14000, Loss 0.0003800\n","Epoch 15000, Loss 0.0003504\n","Epoch 16000, Loss 0.0003250\n","Epoch 17000, Loss 0.0003030\n","Epoch 18000, Loss 0.0002838\n","Epoch 19000, Loss 0.0002669\n","Epoch 20000, Loss 0.0002519\n","Data [0 0], Ground Truth [0] Prediction 0.008439173929876975 Step 0\n","Data [0 1], Ground Truth [1] Prediction 0.9865558483532232 Step 1\n","Data [1 0], Ground Truth [1] Prediction 0.9901585484129376 Step 1\n","Data [1 1], Ground Truth [0] Prediction 0.012445631501320642 Step 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kG_t05n5vGld","colab_type":"text"},"source":["Training the neural network on XOR function with no hidden layers (only the input and output layer, just like a simple perceptron unit)  \n","Here, the neural network fails to learn a non-linear XOR function as predicted"]},{"cell_type":"code","metadata":{"id":"IVoYfGvNt5z-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"ok","timestamp":1601028841883,"user_tz":-390,"elapsed":3501,"user":{"displayName":"Khaing Khant Htun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixmBjV-53DLh5NOtpNtJtqnb5nqLzlfWLyD8OE0g=s64","userId":"00234393451589880378"}},"outputId":"fbfb6e1c-8ed8-4c52-94a2-213e29ad5417"},"source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([[0], [1], [1], [0]])\n","\n","nn_single_layer = NeuralNetwork([2, 1], alpha=0.5)\n","nn_single_layer.fit(X, Y, epoch=20000)\n","\n","for (x, y) in zip(X,Y):\n","\n","  pred = nn_single_layer.predict(x)[0][0]\n","  step = 1 if pred > 0.5 else 0\n","  print(\"Data {}, Ground Truth {} Prediction {} Step {}\".format(x, y, pred, step))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Epoch 1, Loss 0.6289037\n","Epoch 1000, Loss 0.5007938\n","Epoch 2000, Loss 0.5007938\n","Epoch 3000, Loss 0.5007938\n","Epoch 4000, Loss 0.5007938\n","Epoch 5000, Loss 0.5007938\n","Epoch 6000, Loss 0.5007938\n","Epoch 7000, Loss 0.5007938\n","Epoch 8000, Loss 0.5007938\n","Epoch 9000, Loss 0.5007938\n","Epoch 10000, Loss 0.5007938\n","Epoch 11000, Loss 0.5007938\n","Epoch 12000, Loss 0.5007938\n","Epoch 13000, Loss 0.5007938\n","Epoch 14000, Loss 0.5007938\n","Epoch 15000, Loss 0.5007938\n","Epoch 16000, Loss 0.5007938\n","Epoch 17000, Loss 0.5007938\n","Epoch 18000, Loss 0.5007938\n","Epoch 19000, Loss 0.5007938\n","Epoch 20000, Loss 0.5007938\n","Data [0 0], Ground Truth [0] Prediction 0.5161060035862902 Step 1\n","Data [0 1], Ground Truth [1] Prediction 0.5000000000000001 Step 1\n","Data [1 0], Ground Truth [1] Prediction 0.4838939964137099 Step 0\n","Data [1 1], Ground Truth [0] Prediction 0.4678213817930606 Step 0\n"],"name":"stdout"}]}]}